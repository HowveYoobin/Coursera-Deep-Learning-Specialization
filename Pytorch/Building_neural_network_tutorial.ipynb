{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMoSAIX+CvKCZtAwdlOUWQy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 신경망 모델 구성하기\n","\n","* 신경망은 데이터에 대한 연산을 수행하는 계층(layer)/모듈(modeule)로 구성되어 있다.\n","* `torch.nn`: 신경망을 구성하는데 필요한 모든 구성 요소를 제공하는 네임스페이스\n","* `nn.Module`: pytorch의 모든 모듈의 상위 클래스\n","* 신경망은 다른 모듈(계층: layer)로 구성된 모듈이다. 이렇게 중첩한 구조로 복잡한 아키텍처를 쉽게 구축하고 관리할 수 있다.\n","* FashionMNIST 데이터셋의 이미지를 분류하는 신경망을 구성해보자!"],"metadata":{"id":"RosOd1t8pdDe"}},{"cell_type":"code","source":["import os\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms"],"metadata":{"id":"KXhJ_L4OrBgy","executionInfo":{"status":"ok","timestamp":1703938920948,"user_tz":-540,"elapsed":10815,"user":{"displayName":"박유빈학부생","userId":"05269790463358912132"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["## 1. 학습을 위한 장치 얻기\n","* 가능한 경우 GPU 또는 MPS와 같은 하드웨어 가속기에서 모델을 학습.\n","* `torch.cuda` 또는 `torch.backends.mps`가 사용 가능한지 확인해보고, 그렇지 않으면 CPU를 계속 사용"],"metadata":{"id":"ocqLMwNgqPkg"}},{"cell_type":"code","source":["device = (\n","    \"cuda\"\n","    if torch.cuda.is_available()\n","    else \"mps\"\n","    if torch.backends.mps.is_avaiable()\n","    else \"cpu\"\n",")\n","print(f\"Using {device} device\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4cQNwtAaqPZh","executionInfo":{"status":"ok","timestamp":1703938929576,"user_tz":-540,"elapsed":418,"user":{"displayName":"박유빈학부생","userId":"05269790463358912132"}},"outputId":"e2ae9186-9b65-4f19-8883-6b9d6c654294"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda device\n"]}]},{"cell_type":"markdown","source":["## 2. 클래스 정의하기\n","* 신경망 모델을 `nn.Module`의 하위클래스로 정의\n","* `__init__`에서 신경망 계층들을 초기화\n","* `nn.Module`을 상속받은 모든 클래스는 `forward` 메소드에 입력 데이터에 대한 연산들을 구현"],"metadata":{"id":"gqLUcMYjrNA-"}},{"cell_type":"code","execution_count":11,"metadata":{"id":"K4kNBShVpX5T","executionInfo":{"status":"ok","timestamp":1703939872514,"user_tz":-540,"elapsed":2,"user":{"displayName":"박유빈학부생","userId":"05269790463358912132"}}},"outputs":[],"source":["class NeuralNetwork(nn.Module):\n","  def __init__(self):\n","    super().__init__() # super(): 부모클래스의 임시적인 객체를 반환하여 부모클래스의 메소드를 사용할 수 있게 함\n","    self.flatten = nn.Flatten()\n","    self.linear_relu_stack = nn.Sequential(\n","        nn.Linear(28*28, 512),\n","        nn.ReLU(),\n","        nn.Linear(512, 512),\n","        nn.ReLU(),\n","        nn.Linear(512, 10),\n","    )\n","\n","  def forward(self, x):\n","    x = self.flatten(x)\n","    logits = self.linear_relu_stack(x) #logit는 일반적으로 분류 모델에서 출력되는 값들을 나타내는 용어다.\n","    return logits"]},{"cell_type":"markdown","source":["* `NeuralNetwork`의 인스턴스(instance)를 생성하고 이를 `device`로 이동한 뒤, 구조(structure)를 출력"],"metadata":{"id":"sp4E7v2xsS5G"}},{"cell_type":"code","source":["model = NeuralNetwork().to(device)\n","print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nZOCuoWhsPMP","executionInfo":{"status":"ok","timestamp":1703939874681,"user_tz":-540,"elapsed":4,"user":{"displayName":"박유빈학부생","userId":"05269790463358912132"}},"outputId":"30d48adb-d592-4107-943f-a0ebe169eec5"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["NeuralNetwork(\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (linear_relu_stack): Sequential(\n","    (0): Linear(in_features=784, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=512, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=512, out_features=10, bias=True)\n","  )\n",")\n"]}]},{"cell_type":"markdown","source":["* 모델을 사용하기 위해 입력 데이터 전달\n","* 이는 일부 백그라운드 연산들과 함께 모델의 `forward`를 실행한다.\n","* `model.forward()`를 직접 호출하면 안됨\n","\n","* 모델에 입력을 전달하여 호출하면 2차원 텐서 반환\n","* 2차원 텐서의 dim = 0은 각 class에 대한 raw 예측값 10개, dim = 1에는 각 출력의 개별 값들이 해당\n","* 원시 예측값을 `nn.Softmax` 모듈의 인스턴스에 통과시켜 예측 확률을 얻는다."],"metadata":{"id":"Jh8qzHUWsvkd"}},{"cell_type":"code","source":["X = torch.rand(1, 28, 28, device = device)\n","logits = model(X)\n","pred_probab = nn.Softmax(dim = 1)(logits)\n","y_pred = pred_probab.argmax(1)\n","print(f\"Predicted class: {y_pred}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i8YWQmD6sgqQ","executionInfo":{"status":"ok","timestamp":1703939877272,"user_tz":-540,"elapsed":349,"user":{"displayName":"박유빈학부생","userId":"05269790463358912132"}},"outputId":"ba6136da-246c-487a-f2b8-f9a96f445b68"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted class: tensor([9], device='cuda:0')\n"]}]},{"cell_type":"markdown","source":["## 3. 모델 계층 (Layer)\n","* FashionMNIST 모델의 계층들을 살펴보자.\n","* 28 x 28 크기의 이미지 3개로 구성된 미니배치를 가져와 신경망을 통과할 때 어떤 일이 발생하는지 알아보자."],"metadata":{"id":"8s0FsJdAu9kG"}},{"cell_type":"code","source":["input_image = torch.rand(3, 28, 28)\n","print(input_image.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xwJPbiwqtqBf","executionInfo":{"status":"ok","timestamp":1703940007505,"user_tz":-540,"elapsed":3,"user":{"displayName":"박유빈학부생","userId":"05269790463358912132"}},"outputId":"491e8339-dd79-4aab-86f6-6546ccd7a677"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 28, 28])\n"]}]},{"cell_type":"markdown","source":["### nn.Flatten\n","* `nn.Flatten`: 계층을 초기화하여 각 28 X 28의 2D 이미지를 784 픽셀 값을 갖는 연속된 배열로 변환 (dim = 0의 미니배치 차원은 유지)"],"metadata":{"id":"Yhj2hW7pvTIk"}},{"cell_type":"code","source":["flatten = nn.Flatten()\n","flat_image = flatten(input_image)\n","print(flat_image.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OLVWyHMgvSQG","executionInfo":{"status":"ok","timestamp":1703940468229,"user_tz":-540,"elapsed":5,"user":{"displayName":"박유빈학부생","userId":"05269790463358912132"}},"outputId":"c9592b1c-fba0-483f-a346-e3f7d6d9cd62"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 784])\n"]}]},{"cell_type":"markdown","source":["### nn.Linear\n","* 저장된 가중치(w) & 편향(b)를 사용해 선형 변환 (linear transformation)을 적용하는 선형 계층 모듈"],"metadata":{"id":"VhKGDr0vvpYg"}},{"cell_type":"code","source":["layer1 = nn.Linear(in_features = 28 * 28, out_features = 20)\n","hidden1 = layer1(flat_image)\n","print(hidden1.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sjG1XCHRvnUc","executionInfo":{"status":"ok","timestamp":1703940469705,"user_tz":-540,"elapsed":3,"user":{"displayName":"박유빈학부생","userId":"05269790463358912132"}},"outputId":"9fefdd54-aeea-4b3a-d484-64cbc70d5761"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 20])\n"]}]},{"cell_type":"markdown","source":["### nn.ReLU\n","* 선형 변환 이후에 적용되어 nonlinearity 도입 & 신경망이 다양한 현상을 학습할 수 있도록 함\n","* 이 모델에서는 `nn.ReLU`를 선형 계층들 사이에 사용하지만, 모델을 만들 때는 다른 비선형성을 가진 다른 활성화를 도입할 수 있음"],"metadata":{"id":"kA825mJOv_Tj"}},{"cell_type":"code","source":["print(f\"Before ReLU: {hidden1} \\n\\n\")\n","hidden1 = nn.ReLU()(hidden1)\n","print(f\"After ReLU: {hidden1}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zdWWI7DXv-PD","executionInfo":{"status":"ok","timestamp":1703940471396,"user_tz":-540,"elapsed":3,"user":{"displayName":"박유빈학부생","userId":"05269790463358912132"}},"outputId":"c1c41ad0-f7b6-446e-f9de-afe745a798b4"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Before ReLU: tensor([[-0.2764,  0.2738,  0.1709, -0.4190, -0.2762,  0.4262, -0.1862, -0.7226,\n","         -0.1122, -0.1257,  0.1998, -0.8544, -0.5842,  0.3435,  0.0999,  0.0392,\n","         -0.4012, -0.1264,  0.4220,  0.5195],\n","        [-0.4499,  0.3260,  0.1414, -0.5004, -0.3311,  0.2501, -0.1802, -0.5254,\n","         -0.4682, -0.5015, -0.0607, -0.4389, -0.5675,  0.5781,  0.1414,  0.0257,\n","         -0.0195, -0.0135,  0.4963,  0.3422],\n","        [-0.2331,  0.1193, -0.2282, -0.2070, -0.2654,  0.6217, -0.1826, -0.5232,\n","         -0.2234, -0.3016, -0.3195, -0.5898, -0.3203,  0.4328,  0.0721, -0.1271,\n","         -0.3701, -0.3192,  0.6468,  0.4962]], grad_fn=<AddmmBackward0>) \n","\n","\n","After ReLU: tensor([[0.0000, 0.2738, 0.1709, 0.0000, 0.0000, 0.4262, 0.0000, 0.0000, 0.0000,\n","         0.0000, 0.1998, 0.0000, 0.0000, 0.3435, 0.0999, 0.0392, 0.0000, 0.0000,\n","         0.4220, 0.5195],\n","        [0.0000, 0.3260, 0.1414, 0.0000, 0.0000, 0.2501, 0.0000, 0.0000, 0.0000,\n","         0.0000, 0.0000, 0.0000, 0.0000, 0.5781, 0.1414, 0.0257, 0.0000, 0.0000,\n","         0.4963, 0.3422],\n","        [0.0000, 0.1193, 0.0000, 0.0000, 0.0000, 0.6217, 0.0000, 0.0000, 0.0000,\n","         0.0000, 0.0000, 0.0000, 0.0000, 0.4328, 0.0721, 0.0000, 0.0000, 0.0000,\n","         0.6468, 0.4962]], grad_fn=<ReluBackward0>)\n"]}]},{"cell_type":"markdown","source":["### nn.Sequential\n","* `nn.Sequential`: 순서를 갖는 모듈의 컨테이너\n","* 데이터는 정의된 순서로 모든 모듈들을 통해 전달\n","* 순차 컨테이너(sequential container)를 사용해 아래의 `seq_modules`와 같은 신경망을 빠르게 만들 수 있음\n"],"metadata":{"id":"YDUu5aXuxIlp"}},{"cell_type":"code","source":["seq_modules = nn.Sequential(\n","    flatten,\n","    layer1,\n","    nn.ReLU(),\n","    nn.Linear(20, 10)\n",")\n","input_image = torch.rand(3, 28, 28)\n","logits = seq_modules(input_image)"],"metadata":{"id":"4-YTjzfLwi13","executionInfo":{"status":"ok","timestamp":1703940611179,"user_tz":-540,"elapsed":3,"user":{"displayName":"박유빈학부생","userId":"05269790463358912132"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["### nn.Softmax\n","* 신경망의 마지막 선형 계층은 `nn.Softmax`모듈에 전달될 ([-infty, infty]) 범위의 raw value인 `logits`를 반환\n","* `logits`: 모델의 각 class에 대한 예측 확률을 나타내도록 [0,1] 범위로 비례하여 조정(scale)\n","* `dim` parameter: 값의 합이 1이 되는 차원을 나타냄\n"],"metadata":{"id":"I03iEqc0xmnD"}},{"cell_type":"code","source":["softmax = nn.Softmax(dim = 1)\n","pred_probab = softmax(logits)"],"metadata":{"id":"2QvwJ_A9xllb","executionInfo":{"status":"ok","timestamp":1703940751202,"user_tz":-540,"elapsed":4,"user":{"displayName":"박유빈학부생","userId":"05269790463358912132"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["### 모델 매개변수\n","* 신경망 내부의 많은 계층들은 매개변수화(parameterize)된다. -> 학습 중에 최적화되는 가중치와 편향과 이어짐\n","* `nn.Module`을 상속하면 모델 객체 내부의 모든 필드들이 자동으로 추적(track) & 모델의 `parameters()` 및 `named_parameters()` 메소드로 모든 매개변수에 접근 가능\n","\""],"metadata":{"id":"N93tACPcyJGD"}},{"cell_type":"code","source":["print(f)"],"metadata":{"id":"KymbZ5GMyHz_"},"execution_count":null,"outputs":[]}]}