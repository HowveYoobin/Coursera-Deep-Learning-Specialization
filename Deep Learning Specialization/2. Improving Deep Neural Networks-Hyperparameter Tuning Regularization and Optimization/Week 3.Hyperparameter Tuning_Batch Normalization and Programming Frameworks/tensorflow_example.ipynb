{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>\n"
     ]
    }
   ],
   "source": [
    "# Initialize w \n",
    "w = tf.Variable(0, dtype = tf.float32)\n",
    "# Define the optimization algorithm\n",
    "optimizer = tf.keras.optimizers.Adam(0.1)\n",
    "\n",
    "# Loop over \n",
    "def train_step():\n",
    "    # Tensorflow can figure out how to do backpropagation automatically\n",
    "    # GradientTape은 순방향 전파를 계산할 때 조작순서를 기억해놓았다가 역전파의 기울기 계산 때 역순으로 조작 순서를 다시 확인함\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Define the cost function\n",
    "        cost = w ** 2 - 10 * w + 25\n",
    "    # Define trainable variables = list of w\n",
    "    trainable_variables = [w]\n",
    "    # Compute gradients with tape.gradient\n",
    "    grads = tape.gradient(cost, trainable_variables)\n",
    "    # Apply gradients to trainable variables\n",
    "    optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "\n",
    "# w가 처음에 초기화한 값인지 확인: 'numpy=0'\n",
    "print(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.09999931>\n"
     ]
    }
   ],
   "source": [
    "# Implement one step of learning algorithm\n",
    "train_step()\n",
    "# w값이 update되었는지 확인: numpy = 0.1에 가까운 숫자\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.000001>\n"
     ]
    }
   ],
   "source": [
    "# 1000번의 학습\n",
    "for i in range(1000):\n",
    "    train_step()\n",
    "# w updated to nearly 5, which is the minimum of the given cost function\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* w는 최적화하고자 하는 매개변수이므로 변수로 선언\n",
    "* gradient tape를 사용하여 비용함수를 계산하는 데 필요한 작업순서를 기록하기만 하면 tensorflow가 알아서 역전파에 필요한 기울기 계산을 한다.\n",
    "* 최적화하고 싶은 함수는 w의 함수뿐만 아니라 훈련 단계에서의 함수(파라미터가 x인)이기도 하다.\n",
    "* All you need to do is to define how to optimize the cost function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.000001>\n"
     ]
    }
   ],
   "source": [
    "# Initialize w \n",
    "w = tf.Variable(0, dtype = tf.float32)\n",
    "# Define training data x, 3 numbers will play the role as a coefficient of the cost function\n",
    "x = np.array([1.0, -10.0, 25.0], dtype=np.float32)\n",
    "# Define the optimization algorithm\n",
    "optimizer = tf.keras.optimizers.Adam(0.1)\n",
    "\n",
    "def training(x, w, optimizer):\n",
    "    # Define the cost function -> TensorFlow가 computation graph를 만들 수 있게 함\n",
    "    def cost_fn():\n",
    "        # x가 2차 함수의 coefficent 역할\n",
    "        return x[0] * w ** 2 + x[1] * w + x[2]\n",
    "    for i in range(1000):\n",
    "        optimizer.minimize(cost_fn, [w])\n",
    "\n",
    "    return w\n",
    "\n",
    "w = training(x, w, optimizer)\n",
    "print(w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
